/**
 * OpenAIæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ  - OpenAI APIã®èƒ½åŠ›ã‚’æœ€å¤§é™æ´»ç”¨
 * GPT-4ã€Function Callingã€é«˜åº¦ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨
 */

import { openai } from '@/lib/openai';

export interface OpenAIResponse<T> {
  success: boolean;
  data?: T;
  error?: string;
  model: string;
  tokens: {
    prompt: number;
    completion: number;
    total: number;
  };
  processingTime: number;
  quality: {
    confidence: number;
    reasoning: string;
  };
}

export interface OpenAIConfig {
  model: 'gpt-4' | 'gpt-4-turbo' | 'gpt-3.5-turbo';
  temperature: number;
  maxTokens: number;
  timeoutMs: number;
  enableReasoningMode: boolean;
}

export class OpenAIOptimizedSystem {
  private config: OpenAIConfig;

  constructor(config?: Partial<OpenAIConfig>) {
    this.config = {
      model: 'gpt-4', // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§GPT-4ã‚’ä½¿ç”¨
      temperature: 0.7,
      maxTokens: 3000, // ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã‚’å‰Šæ¸›
      timeoutMs: 45000, // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’45ç§’ã«çŸ­ç¸®
      enableReasoningMode: true,
      ...config
    };
  }

  /**
   * é«˜åº¦ãªFunction Calling - OpenAIã®å¼·åŠ›ãªæ©Ÿèƒ½ã‚’æ´»ç”¨
   */
  async executeFunction<T>(
    functionName: string,
    functionSchema: any,
    prompt: string,
    systemMessage?: string,
    options?: Partial<OpenAIConfig>
  ): Promise<OpenAIResponse<T>> {
    const startTime = Date.now();
    const config = { ...this.config, ...options };

    try {
      console.log(`ğŸš€ [OPENAI-OPTIMIZED] Starting ${functionName} with ${config.model}`);
      
      // é«˜åº¦ãªã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
      const enhancedSystemMessage = this.buildEnhancedSystemMessage(
        systemMessage || '', 
        functionName, 
        config
      );

      // OpenAI APIã®å®Ÿè¡Œï¼ˆGPT-3.5ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
      let response;
      try {
        response = await openai.chat.completions.create({
          model: config.model,
          messages: [
            { role: "system", content: enhancedSystemMessage },
            { role: "user", content: prompt }
          ],
          tools: [
            {
              type: "function",
              function: {
                name: functionName,
                description: functionSchema.description,
                parameters: functionSchema.parameters
              }
            }
          ],
          tool_choice: { type: "function", function: { name: functionName } },
          temperature: config.temperature,
          max_tokens: config.maxTokens,
          response_format: { type: "text" },
          timeout: config.timeoutMs
        });
      } catch (error: any) {
        // ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€GPT-3.5-turboã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if (error.message?.includes('tokens') && config.model === 'gpt-4') {
          console.log('âš ï¸ [OPENAI-OPTIMIZED] Token limit exceeded, falling back to GPT-3.5-turbo');
          response = await openai.chat.completions.create({
            model: 'gpt-3.5-turbo',
            messages: [
              { role: "system", content: enhancedSystemMessage },
              { role: "user", content: prompt }
            ],
            tools: [
              {
                type: "function",
                function: {
                  name: functionName,
                  description: functionSchema.description,
                  parameters: functionSchema.parameters
                }
              }
            ],
            tool_choice: { type: "function", function: { name: functionName } },
            temperature: config.temperature,
            max_tokens: Math.min(config.maxTokens, 3000),
            response_format: { type: "text" },
            timeout: config.timeoutMs
          });
        } else {
          throw error;
        }
      }

      // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è§£æ
      const toolCall = response.choices[0]?.message?.tool_calls?.[0];
      if (!toolCall?.function?.arguments) {
        throw new Error('No valid function call response');
      }

      // JSONå¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆå …ç‰¢ãªè§£æå‡¦ç†ï¼‰
      console.log('ğŸ“¥ [OPENAI-OPTIMIZED] Parsing function arguments');
      
      let rawArguments = toolCall.function.arguments;
      console.log('ğŸ” [OPENAI-OPTIMIZED] Raw arguments length:', rawArguments.length);
      
      let parsedData: T;
      
      try {
        // ç›´æ¥è§£æã‚’è©¦è¡Œ
        parsedData = JSON.parse(rawArguments);
        console.log('âœ… [OPENAI-OPTIMIZED] Direct JSON parse successful');
      } catch (directError) {
        console.log('âš ï¸ [OPENAI-OPTIMIZED] Direct parse failed, trying cleanup');
        console.log('ğŸ” [OPENAI-OPTIMIZED] Raw arguments length:', rawArguments.length);
        
        // ã‚ˆã‚Šå …ç‰¢ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†
        let cleanedArguments = rawArguments
          // ãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆã®å•é¡Œã‚’æ ¹æœ¬çš„ã«è§£æ±º
          .replace(/`+/g, '"')                  // å…¨ã¦ã®ãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆã‚’"ã«ç½®æ›
          .replace(/```[\w]*\n?/g, '')          // ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãƒãƒ¼ã‚«ãƒ¼é™¤å»
          .replace(/'/g, '"')                   // ã‚·ãƒ³ã‚°ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã‚’ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã«
          .replace(/[\r\n\t]/g, ' ')            // æ”¹è¡Œã¨ã‚¿ãƒ–ã‚’ç©ºç™½ã«
          .replace(/\s+/g, ' ')                 // é€£ç¶šã™ã‚‹ç©ºç™½ã‚’1ã¤ã«
          .replace(/,(\s*[}\]])/g, '$1')        // trailing commaå‰Šé™¤ï¼ˆã‚ˆã‚Šæ­£ç¢ºï¼‰
          .replace(/([{,]\s*)"([^"]*)"(\s*:)/g, '$1"$2"$3') // ã‚­ãƒ¼ã®æ­£è¦åŒ–
          .trim();
        
        console.log('ğŸ”§ [OPENAI-OPTIMIZED] Cleaned arguments length:', cleanedArguments.length);
        
        try {
          parsedData = JSON.parse(cleanedArguments);
          console.log('âœ… [OPENAI-OPTIMIZED] Cleaned JSON parse successful');
        } catch (cleanError) {
          console.log('âš ï¸ [OPENAI-OPTIMIZED] Cleanup failed, trying advanced extraction');
          
          // ã‚ˆã‚Šå¼·åŠ›ãªJSONæŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³
          const jsonPatterns = [
            // ãƒ‘ã‚¿ãƒ¼ãƒ³1: æœ€ã‚‚å¤–å´ã®{}ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£
            /^[^{]*(\{[\s\S]*\})[^}]*$/,
            // ãƒ‘ã‚¿ãƒ¼ãƒ³2: ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®JSON
            /```(?:json)?\s*(\{[\s\S]*?\})\s*```/i,
            // ãƒ‘ã‚¿ãƒ¼ãƒ³3: åŸºæœ¬çš„ãª{}ãƒ–ãƒ­ãƒƒã‚¯
            /(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})/,
            // ãƒ‘ã‚¿ãƒ¼ãƒ³4: ã‚·ãƒ³ãƒ—ãƒ«ãªå˜ä¸€ãƒ¬ãƒ™ãƒ«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            /\{[^{}]*\}/
          ];
          
          let extracted: string | null = null;
          for (let i = 0; i < jsonPatterns.length; i++) {
            const pattern = jsonPatterns[i];
            const match = rawArguments.match(pattern);
            if (match) {
              extracted = match[1] || match[0];
              // åŒã˜ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é©ç”¨
              extracted = extracted
                .replace(/`+/g, '"')
                .replace(/'/g, '"')
                .replace(/[\r\n\t]/g, ' ')
                .replace(/\s+/g, ' ')
                .replace(/,(\s*[}\]])/g, '$1')
                .trim();
              
              try {
                parsedData = JSON.parse(extracted);
                console.log(`âœ… [OPENAI-OPTIMIZED] Pattern ${i+1} extraction successful`);
                break;
              } catch (e) {
                console.log(`âš ï¸ [OPENAI-OPTIMIZED] Pattern ${i+1} failed:`, (e as Error).message);
                continue;
              }
            }
          }
          
          if (!parsedData) {
            console.error('âŒ [OPENAI-OPTIMIZED] All parsing attempts failed');
            console.error('Raw arguments sample:', rawArguments.substring(0, 500));
            console.error('Cleaned sample:', cleanedArguments.substring(0, 500));
            throw new Error(`JSON parsing failed: ${(cleanError as Error).message}`);
          }
        }
      }
      
      // å“è³ªè©•ä¾¡
      const quality = this.evaluateResponseQuality(parsedData, functionSchema);
      
      const result: OpenAIResponse<T> = {
        success: true,
        data: parsedData,
        model: config.model,
        tokens: {
          prompt: response.usage?.prompt_tokens || 0,
          completion: response.usage?.completion_tokens || 0,
          total: response.usage?.total_tokens || 0
        },
        processingTime: Date.now() - startTime,
        quality
      };

      console.log(`âœ… [OPENAI-OPTIMIZED] ${functionName} completed successfully`);
      console.log(`ğŸ“Š [OPENAI-OPTIMIZED] Tokens: ${result.tokens.total}, Quality: ${quality.confidence}`);
      
      return result;

    } catch (error) {
      console.error(`âŒ [OPENAI-OPTIMIZED] ${functionName} failed:`, error);
      
      return {
        success: false,
        error: error instanceof Error ? error.message : 'Unknown error',
        model: config.model,
        tokens: { prompt: 0, completion: 0, total: 0 },
        processingTime: Date.now() - startTime,
        quality: { confidence: 0, reasoning: 'Function execution failed' }
      };
    }
  }

  /**
   * é«˜åº¦ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ - Creative Writingã€åˆ†æã€æ¨è«–ã«æœ€é©
   */
  async generateAdvancedText(
    prompt: string,
    type: 'creative' | 'analytical' | 'reasoning' | 'technical',
    options?: Partial<OpenAIConfig>
  ): Promise<OpenAIResponse<string>> {
    const startTime = Date.now();
    const config = { ...this.config, ...options };

    // ã‚¿ã‚¤ãƒ—åˆ¥ã®æœ€é©åŒ–
    const typeOptimization = this.getTypeOptimization(type);
    config.temperature = typeOptimization.temperature;
    config.model = typeOptimization.preferredModel;

    try {
      console.log(`ğŸ¨ [OPENAI-OPTIMIZED] Generating ${type} text with ${config.model}`);

      let response;
      try {
        response = await openai.chat.completions.create({
          model: config.model,
          messages: [
            { role: "system", content: typeOptimization.systemPrompt },
            { role: "user", content: prompt }
          ],
          temperature: config.temperature,
          max_tokens: config.maxTokens,
          presence_penalty: typeOptimization.presencePenalty,
          frequency_penalty: typeOptimization.frequencyPenalty,
          timeout: config.timeoutMs
        });
      } catch (error: any) {
        // ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€GPT-3.5-turboã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if (error.message?.includes('tokens') && config.model === 'gpt-4') {
          console.log('âš ï¸ [OPENAI-OPTIMIZED] Token limit exceeded, falling back to GPT-3.5-turbo');
          response = await openai.chat.completions.create({
            model: 'gpt-3.5-turbo',
            messages: [
              { role: "system", content: typeOptimization.systemPrompt },
              { role: "user", content: prompt }
            ],
            temperature: config.temperature,
            max_tokens: Math.min(config.maxTokens, 3000),
            presence_penalty: typeOptimization.presencePenalty,
            frequency_penalty: typeOptimization.frequencyPenalty,
            timeout: config.timeoutMs
          });
        } else {
          throw error;
        }
      }

      const content = response.choices[0]?.message?.content || '';
      
      // ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªè©•ä¾¡
      const quality = this.evaluateTextQuality(content, type);

      const result: OpenAIResponse<string> = {
        success: true,
        data: content,
        model: config.model,
        tokens: {
          prompt: response.usage?.prompt_tokens || 0,
          completion: response.usage?.completion_tokens || 0,
          total: response.usage?.total_tokens || 0
        },
        processingTime: Date.now() - startTime,
        quality
      };

      console.log(`âœ… [OPENAI-OPTIMIZED] ${type} text generated successfully`);
      
      return result;

    } catch (error) {
      console.error(`âŒ [OPENAI-OPTIMIZED] ${type} text generation failed:`, error);
      
      return {
        success: false,
        error: error instanceof Error ? error.message : 'Unknown error',
        model: config.model,
        tokens: { prompt: 0, completion: 0, total: 0 },
        processingTime: Date.now() - startTime,
        quality: { confidence: 0, reasoning: 'Text generation failed' }
      };
    }
  }

  /**
   * å¼·åŒ–ã•ã‚ŒãŸã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ§‹ç¯‰
   */
  private buildEnhancedSystemMessage(
    baseMessage: string, 
    functionName: string, 
    config: OpenAIConfig
  ): string {
    const enhancementPrompts = {
      reasoning: config.enableReasoningMode ? `
æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’æ˜ç¢ºã«ã—ã€æ®µéšçš„ã«æ¨è«–ã—ã¦ãã ã•ã„ã€‚
å„æ±ºå®šã®æ ¹æ‹ ã‚’ç¤ºã—ã€ä»£æ›¿æ¡ˆã‚‚è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚` : '',
      
      quality: `
æœ€é«˜å“è³ªã®çµæœã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚ä»¥ä¸‹ã®åŸºæº–ã‚’æº€ãŸã™ã‚ˆã†åŠªã‚ã¦ãã ã•ã„ï¼š
- æ­£ç¢ºæ€§ã¨ä¸€è²«æ€§
- è©³ç´°ã§å…·ä½“çš„ãªå†…å®¹
- å®Ÿç”¨çš„ã§å®Ÿè£…å¯èƒ½ãªææ¡ˆ
- å‰µé€ æ€§ã¨é©æ–°æ€§ã®ãƒãƒ©ãƒ³ã‚¹`,
      
      structure: `
æ§‹é€ åŒ–ã•ã‚ŒãŸå½¢å¼ã§å¿œç­”ã—ã€ã™ã¹ã¦ã®å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
JSONã‚¹ã‚­ãƒ¼ãƒã«å®Œå…¨ã«æº–æ‹ ã—ã€å‹å®‰å…¨æ€§ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„ã€‚`
    };

    return `${baseMessage}

${enhancementPrompts.reasoning}

${enhancementPrompts.quality}

${enhancementPrompts.structure}

Function: ${functionName}
Model: ${config.model}
Quality Mode: MAXIMUM`;
  }

  /**
   * ãƒ¬ã‚¹ãƒãƒ³ã‚¹å“è³ªã®è©•ä¾¡
   */
  private evaluateResponseQuality(data: any, schema: any): { confidence: number; reasoning: string } {
    let confidence = 0.8; // Base confidence for GPT-4
    const issues: string[] = [];

    // å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
    const required = schema.parameters?.required || [];
    const missingFields = required.filter((field: string) => !(field in data));
    if (missingFields.length === 0) {
      confidence += 0.1;
    } else {
      issues.push(`Missing fields: ${missingFields.join(', ')}`);
      confidence -= 0.2;
    }

    // ãƒ‡ãƒ¼ã‚¿ã®è±Šå¯Œã•è©•ä¾¡
    const dataString = JSON.stringify(data);
    if (dataString.length > 500) confidence += 0.05;
    if (dataString.length > 1000) confidence += 0.05;

    // ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®è¤‡é›‘ã•è©•ä¾¡
    if (typeof data === 'object' && data !== null) {
      const keys = Object.keys(data);
      if (keys.length >= 5) confidence += 0.1;
      if (keys.length >= 8) confidence += 0.05;
    }

    const reasoning = issues.length > 0 
      ? `Quality issues: ${issues.join('; ')}` 
      : 'High quality response with all requirements met';

    return { 
      confidence: Math.min(Math.max(confidence, 0), 1), 
      reasoning 
    };
  }

  /**
   * ãƒ†ã‚­ã‚¹ãƒˆå“è³ªã®è©•ä¾¡
   */
  private evaluateTextQuality(content: string, type: string): { confidence: number; reasoning: string } {
    let confidence = 0.8;
    const factors: string[] = [];

    // é•·ã•ã®è©•ä¾¡
    if (content.length > 200) {
      confidence += 0.05;
      factors.push('adequate length');
    }
    if (content.length > 500) {
      confidence += 0.05;
      factors.push('comprehensive content');
    }

    // æ§‹é€ ã®è©•ä¾¡
    const paragraphs = content.split('\n\n').length;
    if (paragraphs >= 2) {
      confidence += 0.05;
      factors.push('well-structured');
    }

    // ã‚¿ã‚¤ãƒ—åˆ¥è©•ä¾¡
    switch (type) {
      case 'creative':
        if (content.includes('!') || content.includes('?')) {
          confidence += 0.05;
          factors.push('expressive language');
        }
        break;
      case 'analytical':
        if (content.includes('åˆ†æ') || content.includes('è©•ä¾¡')) {
          confidence += 0.05;
          factors.push('analytical depth');
        }
        break;
      case 'technical':
        if (content.includes('å®Ÿè£…') || content.includes('æŠ€è¡“')) {
          confidence += 0.05;
          factors.push('technical accuracy');
        }
        break;
    }

    const reasoning = factors.length > 0 
      ? `Quality factors: ${factors.join(', ')}` 
      : 'Standard quality text generated';

    return { 
      confidence: Math.min(confidence, 1), 
      reasoning 
    };
  }

  /**
   * ç”Ÿæˆã‚¿ã‚¤ãƒ—åˆ¥ã®æœ€é©åŒ–è¨­å®š
   */
  private getTypeOptimization(type: string) {
    const optimizations = {
      creative: {
        temperature: 0.9,
        preferredModel: 'gpt-4' as const,
        presencePenalty: 0.6,
        frequencyPenalty: 0.3,
        systemPrompt: `ã‚ãªãŸã¯å‰µé€ çš„ã§é©æ–°çš„ãªã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
ç‹¬å‰µæ€§ã€é­…åŠ›ã€å®Ÿç”¨æ€§ã‚’å…¼ã­å‚™ãˆãŸå†…å®¹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æƒ³åƒåŠ›ã‚’åˆºæ¿€ã—ã€æ–°ã—ã„è¦–ç‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚`
      },
      analytical: {
        temperature: 0.3,
        preferredModel: 'gpt-4' as const,
        presencePenalty: 0.1,
        frequencyPenalty: 0.1,
        systemPrompt: `ã‚ãªãŸã¯è©³ç´°ãªåˆ†æã¨è«–ç†çš„æ€è€ƒã‚’è¡Œã†å°‚é–€å®¶ã§ã™ã€‚
ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ãŸæ´å¯Ÿã€æ˜ç¢ºãªè«–æ‹ ã€å®Ÿè¨¼å¯èƒ½ãªçµè«–ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
è¤‡é›‘ãªå•é¡Œã‚’ä½“ç³»çš„ã«åˆ†è§£ã—ã€åŒ…æ‹¬çš„ãªåˆ†æã‚’è¡Œã£ã¦ãã ã•ã„ã€‚`
      },
      reasoning: {
        temperature: 0.2,
        preferredModel: 'gpt-4' as const,
        presencePenalty: 0.0,
        frequencyPenalty: 0.0,
        systemPrompt: `ã‚ãªãŸã¯æ®µéšçš„æ¨è«–ã¨è«–ç†çš„æ€è€ƒã‚’è¡Œã†å°‚é–€å®¶ã§ã™ã€‚
å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ˜ç¢ºã«ç¤ºã—ã€å‰ææ¡ä»¶ã‹ã‚‰çµè«–ã¾ã§è«–ç†çš„ã«å°ã„ã¦ãã ã•ã„ã€‚
ä»£æ›¿æ¡ˆã‚‚è€ƒæ…®ã—ã€æœ€ã‚‚åˆç†çš„ãªè§£æ±ºç­–ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚`
      },
      technical: {
        temperature: 0.1,
        preferredModel: 'gpt-4' as const,
        presencePenalty: 0.0,
        frequencyPenalty: 0.0,
        systemPrompt: `ã‚ãªãŸã¯æŠ€è¡“çš„ç²¾åº¦ã¨å®Ÿè£…å¯èƒ½æ€§ã‚’é‡è¦–ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
æ­£ç¢ºãªæŠ€è¡“æƒ…å ±ã€ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã€å®Ÿè£…å¯èƒ½ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
ã‚³ãƒ¼ãƒ‰ã®å“è³ªã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ€§ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚`
      }
    };

    return optimizations[type as keyof typeof optimizations] || optimizations.analytical;
  }

  /**
   * ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
   */
  async healthCheck(): Promise<{
    available: boolean;
    model: string;
    latency: number;
    capabilities: string[];
  }> {
    const start = Date.now();
    
    try {
      const response = await openai.chat.completions.create({
        model: this.config.model,
        messages: [{ role: "user", content: "Health check" }],
        max_tokens: 5
      });

      return {
        available: true,
        model: this.config.model,
        latency: Date.now() - start,
        capabilities: [
          'Function Calling',
          'Advanced Reasoning',
          'Creative Generation',
          'Technical Analysis',
          'Multi-language Support'
        ]
      };
    } catch (error) {
      return {
        available: false,
        model: this.config.model,
        latency: Date.now() - start,
        capabilities: []
      };
    }
  }
}

export const openAIOptimized = new OpenAIOptimizedSystem();